{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13592788,"sourceType":"datasetVersion","datasetId":8636464}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Install required packages (run once)\n# Adjust / remove if already installed\n!pip install -q pandas tqdm transformers sentencepiece sacremoses fasttext langdetect sentence_transformers rapidfuzz accelerate\n!pip install -q \"huggingface_hub>=0.17.0\"\n\n# Optional high-quality Indic translation (if you want to try later)\n# !pip install -q git+https://github.com/AI4Bharat/indic-trans.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: Imports and config\nimport os, glob, json, re\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport pandas as pd\nfrom langdetect import detect, DetectorFactory\nDetectorFactory.seed = 0\nfrom rapidfuzz import fuzz\nfrom rapidfuzz import process as rfp\nfrom transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# Make sure GPU is available\nDEVICE = 0 if torch.cuda.is_available() else -1\nprint(\"Using device:\", \"cuda:0\" if DEVICE==0 else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3: User-config - folders and files\n\nRAW_FOLDER = \"/kaggle/input/finetuningdataforagroshakti\"     # Folder containing all CSV and parquet files\nOUT_FOLDER = \"/kaggle/working/cleaned_output\"\nos.makedirs(OUT_FOLDER, exist_ok=True)\n\n# Glob input CSVs for CropQueryDataset files\ncsv_files = sorted(glob.glob(os.path.join(RAW_FOLDER, \"*CropQueryDataset*.csv\")))\n# parquet_files = sorted(glob.glob(os.path.join(RAW_FOLDER, \"*CropQueryDataset*.parquet\")))\n\nprint(\"Found CSV files:\", len(csv_files))\nfor f in csv_files:\n    print(\"  \", os.path.basename(f))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: Columns selection and standardization\n# We'll map incoming columns to canonical names and only keep the selected ones\nkeep_cols = {\n    \"StateName\": \"state\",\n    \"DistrictName\": \"district\",\n    \"Category\": \"category\",\n    \"Crop\": \"crop\",\n    \"QueryType\": \"query_type\",\n    \"QueryText\": \"query\",\n    \"KccAns\": \"answer\",\n    \"year\": \"year\",\n    \"month\": \"month\"\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5: Load and concat CSVs (with basic cleaning of column names)\ndfs = []\nfor file in csv_files:\n    df = pd.read_csv(file, dtype=str, keep_default_na=False, na_values=[\"\", \"NA\", \"None\"], engine='python',\n    on_bad_lines='skip')\n    # Normalize column names\n    df.columns = [c.strip() for c in df.columns]\n    # Keep only available mapping columns\n    available = {k:v for k,v in keep_cols.items() if k in df.columns}\n    df = df[list(available.keys())].rename(columns=available)\n    dfs.append(df)\n\ndf = pd.concat(dfs, ignore_index=True, sort=False)\nprint(\"Combined rows:\", len(df))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6: Quick stats & drop rows with missing query/answer\nprint(\"Before dropna:\", len(df))\ndf = df[df['query'].notna() & (df['query'].str.strip() != \"\")]\ndf = df[df['answer'].notna() & (df['answer'].str.strip() != \"\")]\nprint(\"After dropping null queries/answers:\", len(df))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7: Text cleaning helper functions\n# Regex-based cleaning, keep language-aware punctuation\ndef basic_clean_text(text):\n    if not isinstance(text, str):\n        return \"\"\n    t = text.strip()\n    # remove odd unicode like zero-width etc\n    t = re.sub(r'[\\u200b-\\u200f\\uFEFF]', '', t)\n    # Replace multiple spaces/newlines\n    t = re.sub(r'\\s+', ' ', t)\n    # Remove URLs and emails\n    t = re.sub(r'http\\S+|www\\.\\S+|[\\w\\.-]+@[\\w\\.-]+', '', t)\n    # Remove long numeric IDs etc\n    t = re.sub(r'\\b\\d{6,}\\b', '', t)\n    # Remove emojis and non-text symbols (keep basic punctuation)\n    t = re.sub(r'[^\\w\\s\\.\\,\\?\\!\\-\\/\\u0900-\\u097F]', '', t)  # allow Devanagari range\n    t = t.strip(\" -,.\")\n    return t\n\n# Normalize whitespace & punctuation for queries & answers\ndf['query'] = df['query'].map(basic_clean_text)\ndf['answer'] = df['answer'].map(basic_clean_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8: Remove obvious non-agri rows via keyword blacklist (extend as needed)\n# This is conservative: remove rows containing bank/account/complaint terms\nnon_agri_keywords = [\n    \"bank\", \"loan\", \"atm\", \"mobile recharge\", \"complaint\", \"registration\", \"adhar\", \"aadhar\",\n    \"helpline number\", \"phone is not working\", \"electricity bill\", \"police\", \"payment\", \"hanged\",\n    \"HANGED\", \"BANK\", \"LOAN\"\n]\npattern = re.compile(\"|\".join(re.escape(k) for k in non_agri_keywords), flags=re.IGNORECASE)\ndf = df[~df['query'].str.contains(pattern, na=False)]\nprint(\"After non-agri filter:\", len(df))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install datasketch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10: Deduplication - exact then fast near-duplicate dedupe using MinHash LSH\nfrom datasketch import MinHash, MinHashLSH\n\n# Fast MinHash + LSH dedupe (replaces fuzzy dedupe)\ndef minhash_lsh_dedupe(df, text_col='query', threshold=0.92, num_perm=64):\n    \"\"\"\n    threshold: similarity threshold (0.8–0.95 recommended)\n    num_perm: number of hash permutations (64 = fast, 128 = more accurate)\n    \"\"\"\n    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n    keep_idx = []\n    minhashes = {}\n\n    for idx, text in tqdm(df[text_col].fillna(\"\").items(),\n                          total=len(df), desc=\"MinHash LSH Dedupe\"):\n        # Create MinHash\n        m = MinHash(num_perm=num_perm)\n        for word in str(text).lower().split():\n            m.update(word.encode('utf8'))\n\n        # Check if similar already exists\n        result = lsh.query(m)\n        if not result:         # no near-duplicate found → keep this row\n            lsh.insert(idx, m)\n            keep_idx.append(idx)\n\n    return df.loc[keep_idx]\n\n\n# Run fast dedupe\nbefore = len(df)\ndf = minhash_lsh_dedupe(df, text_col='query', threshold=0.95, num_perm=64)\nprint(\"Removed near-duplicates:\", before - len(df))\nprint(\"After dedupe:\", len(df))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-18T11:32:32.903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-18T11:32:32.904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(df)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-18T11:32:32.904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install fasttext\n!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import fasttext\nimport tqdm\nimport pandas\n\n# Load fastText language ID model\nlang_model = fasttext.load_model(\"lid.176.bin\")\n\ndef detect_lang_fast(text):\n    if not isinstance(text, str) or len(text.strip()) < 3:\n        return \"unknown\"\n\n    # Detect Indic scripts first (more accurate than model on romanized text)\n    if re.search(r'[\\u0900-\\u097F]', text): return 'hi'   # Hindi/Marathi/Nepali\n    if re.search(r'[\\u0A00-\\u0A7F]', text): return 'pa'   # Punjabi\n    if re.search(r'[\\u0980-\\u09FF]', text): return 'bn'   # Bengali\n    if re.search(r'[\\u0C80-\\u0CFF]', text): return 'kn'   # Kannada\n\n    # FastText expects at least 3 characters of text\n    try:\n        pred = lang_model.predict(text.replace(\"\\n\", \" \")[:200])[0][0]\n        return pred.replace(\"__label__\", \"\")  # example: \"__label__en\" -> \"en\"\n    except:\n        return \"unknown\"\n\n\ntqdm.pandas()\ndf['a_lang'] = df['answer'].progress_apply(detect_lang_fast)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[df['a_lang'].isin(['hi','en'])]\n\nprint('new size',len(df))\n\nprint(df['a_lang'].unique())\nprint(df['a_lang'].nunique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install IndicTransToolkit huggingface_hub transformers --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nlogin('')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom IndicTransToolkit.processor import IndicProcessor\n\n# -------------------------------\n# 1. Load your dataset\n# -------------------------------\n# Change 'your_dataset.csv' to your actual file name\ndf = pd.read_csv(\"/kaggle/working/filtered_halfpreprocessed.csv\")\n\n# Make sure there's a column named 'answer'\nif \"answer\" not in df.columns:\n    raise ValueError(\"Dataset must have a column named 'answer'\")\n\n# -------------------------------\n# 2. Setup translation model\n# -------------------------------\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Hindi → English model\nmodel_name = \"ai4bharat/indictrans2-indic-en-1B\"\nsrc_lang, tgt_lang = \"hin_Deva\", \"eng_Latn\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n).to(DEVICE)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ip = IndicProcessor(inference=True)\n\n# -------------------------------\n# 3. Define translation function\n# -------------------------------\ndef translate_hindi_to_english(sentence):\n    if not isinstance(sentence, str) or sentence.strip() == \"\":\n        return \"\"\n\n    try:\n        # detect language\n        lang = detect(sentence)\n\n        if lang != \"hi\":  # If not Hindi, return as is\n            return sentence\n\n        # preprocess\n        batch = ip.preprocess_batch([sentence], src_lang=src_lang, tgt_lang=tgt_lang)\n        inputs = tokenizer(batch, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(DEVICE)\n\n        with torch.no_grad():\n            generated_tokens = model.generate(\n                **inputs,\n                use_cache=False,\n                max_length=256,\n                num_beams=5,\n                num_return_sequences=1,\n            )\n\n        generated_tokens = tokenizer.batch_decode(\n            generated_tokens,\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True,\n        )\n\n        translation = ip.postprocess_batch(generated_tokens, lang=tgt_lang)[0]\n        return translation\n    except Exception as e:\n        print(f\"Error translating: {sentence}\\n{e}\")\n        return sentence\n\n# -------------------------------\n# 4. Apply translation on the dataset\n# -------------------------------\ndf[\"translated\"] = df[\"answer\"].apply(translate_hindi_to_english)\n\n# -------------------------------\n# 5. Save the result\n# -------------------------------\ndf.to_csv(\"translated_dataset.csv\", index=False)\nprint(\"✅ Translation completed. Saved as translated_dataset.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install truecase","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import truecase\n\ndf['state'] = df['state'].apply(lambda x: truecase.get_true_case(x))\ndf['district'] = df['district'].apply(lambda x: truecase.get_true_case(x))\ndf['query'] = df['query'].apply(lambda x: truecase.get_true_case(x))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install IndicTransToolkit --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom IndicTransToolkit.processor import IndicProcessor\nfrom tqdm import tqdm\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL = \"ai4bharat/indictrans2-indic-en-dist-200M\"\n\ntok = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    MODEL, trust_remote_code=True,\n    torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\"\n).to(DEVICE)\n\nip = IndicProcessor(inference=True)\n\ndef translate_df(df, state, src_lang, batch_size):\n    mask = (df[\"state\"] == state) & (df[\"a_lang\"] == \"hi\")\n    texts = df.loc[mask, \"answer\"].tolist()\n    results = []\n\n    for i in tqdm(range(0, len(texts), batch_size), desc=state):\n        batch = ip.preprocess_batch(\n            texts[i:i+batch_size], src_lang=src_lang, tgt_lang=\"eng_Latn\"\n        )\n        inp = tok(batch, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n        inp = {k: v.to(DEVICE, non_blocking=True) for k, v in inp.items()}\n\n        with torch.no_grad():\n            out = model.generate(**inp, num_beams=1, max_length=128,\n                                 no_repeat_ngram_size=2, early_stopping=True)\n\n        trans = ip.postprocess_batch(\n            tok.batch_decode(out, skip_special_tokens=True), lang=\"eng_Latn\"\n        )\n        results.extend(trans)\n\n        del inp, out, batch, trans\n        torch.cuda.empty_cache()\n\n    df.loc[mask, \"answer\"] = results\n\n\n# ------------------ CONFIG ------------------\nconfigs = [\n    (\"Haryana\", \"pan_Guru\", 32),\n    (\"West Bengal\", \"ben_Beng\", 64),\n    (\"Karnataka\", \"kan_Knda\", 32),\n    (\"Punjab\", \"pan_Guru\", 128),\n]\n\nfor state, lang, bs in configs:\n    translate_df(df, state, lang, bs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask = (df['state'] == 'Punjab') & (df['a_lang'] == 'hi')\ndf.loc[mask, 'translated'] = df.loc[mask, 'answer']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"min_tokens = 6\ndf = df[df['translated'].str.split().str.len() >= min_tokens]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(columns=['answer', 'year', 'a_lang'], inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.rename(columns={'translated': 'answer'}, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cols = ['query_type', 'query', 'answer']\n\nfor col in cols:\n    df[col] = df[col].astype(str).str.strip()             # remove leading/trailing spaces\n    df[col] = df[col].str.replace(r'\\s+', ' ', regex=True)  # replace multiple spaces with single space","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[df['query_type'] != 'Weather']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for qt, count in df['query_type'].value_counts().items():\n    print(f\"{qt} ------ {count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = df['query_type'].unique()\nfor i in x:\n    print(i)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"crops = df['crop'].unique()\nfor c in crops:\n    count = df['crop'].value_counts().loc[c]\n    print(f\"{c} ------ {count}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv('/kaggle/working/translated_dataset_final6.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}